{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Findability\n",
    "\n",
    "The goal is to make your dataset easy to identify and search for by both humans and machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/NOx_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.columns[:-6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('NOx.h5', 'w') as hdf:\n",
    "    # meta_group = hdf.create_group(\"metadata\", track_order=True)\n",
    "    hdf.attrs[\"ID\"] = \"smoke_2024_10_01\"\n",
    "    hdf.attrs[\"title\"] = \"Smoke Data of 2024-10-01\"\n",
    "    hdf.attrs[\"description\"] = \"This dataset contains the NOx and Effectiveness data for further ML\"\n",
    "    hdf.attrs[\"author\"] = \"D. Druzhbin\"\n",
    "    hdf.attrs[\"email\"] = \"\"\n",
    "    hdf.attrs[\"organization\"] = \"Halias Technologies\"\n",
    "    hdf.attrs[\"keywords\"] = \"NOx, Effectiveness, Smoke, 2024-10-01, ML\"\n",
    "    hdf.attrs[\"date_created\"] = \"2024-10-01\"\n",
    "    hdf.attrs[\"data_format\"] = \"hdf5\"\n",
    "    hdf.attrs[\"license\"] = \"\"\n",
    "\n",
    "    sensor_readings = hdf.create_dataset(\"sensor_readings\", data=data.values, dtype='float32')\n",
    "    sensor_readings.attrs[\"columns\"] = \", \".join(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O2, CO, NOx, T.Gas, T.Air, Draught, CO2, Eff., Losses, Excess Air, Dew point, T. Sensor, T1 DeltaT, T2 DeltaT, Air pressure, I.Flow\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('NOx.h5', 'r') as hdf:\n",
    "    sensor_readings = hdf['sensor_readings']\n",
    "    print(sensor_readings.attrs['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global Metadata:\n",
      "ID: smoke_2024_10_01\n",
      "author: D. Druzhbin\n",
      "data_format: hdf5\n",
      "date_created: 2024-10-01\n",
      "description: This dataset contains the NOx and Effectiveness data for further ML\n",
      "email: \n",
      "keywords: NOx, Effectiveness, Smoke, 2024-10-01, ML\n",
      "license: \n",
      "organization: Halias Technologies\n",
      "title: Smoke Data of 2024-10-01\n",
      "\n",
      "Dataset Metadata:\n",
      "Columns: O2, CO, NOx, T.Gas, T.Air, Draught, CO2, Eff., Losses, Excess Air, Dew point, T. Sensor, T1 DeltaT, T2 DeltaT, Air pressure, I.Flow\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"NOx.h5\", \"r\") as hdf:\n",
    "    print(\"\\nGlobal Metadata:\")\n",
    "    for key, value in hdf.attrs.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(\"\\nDataset Metadata:\")\n",
    "    dataset = hdf[\"sensor_readings\"]\n",
    "    print(\"Columns:\", dataset.attrs[\"columns\"])  # Retrieve column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking FAIR Compliance for: NOx.h5\n",
      "\n",
      "Global Metadata Verification:\n",
      "ID: smoke_2024_10_01\n",
      "title: Smoke Data of 2024-10-01\n",
      "description: This dataset contains the NOx and Effectiveness data for further ML\n",
      "author: D. Druzhbin\n",
      "organization: Halias Technologies\n",
      "keywords: NOx, Effectiveness, Smoke, 2024-10-01, ML\n",
      "date_created: 2024-10-01\n",
      "data_format: hdf5\n",
      "license: \n",
      "\n",
      "Dataset Verification:\n",
      "Dataset 'sensor_readings' found.\n",
      "Columns: O2, CO, NOx, T.Gas, T.Air, Draught, CO2, Eff., Losses, Excess Air, Dew point, T. Sensor, T1 DeltaT, T2 DeltaT, Air pressure, I.Flow\n",
      "\n",
      "Data Integrity Check:\n",
      "SHA256 Checksum: ef6174db00a5a406e3fca84fba8283961359f1e24d69a07f388a895102fd8107\n",
      "\n",
      "FAIR Compliance Report:\n",
      "The dataset meets FAIR principles!\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import hashlib\n",
    "\n",
    "def compute_checksum(file_path):\n",
    "    \"\"\"Compute SHA256 checksum of the file to ensure data integrity.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "def check_fair_compliance(hdf5_file):\n",
    "    \"\"\"Checks FAIR compliance for an HDF5 dataset.\"\"\"\n",
    "    required_metadata = [\n",
    "        \"ID\", \"title\", \"description\", \"author\", \"organization\",\n",
    "        \"keywords\", \"date_created\", \"data_format\", \"license\"\n",
    "    ]\n",
    "    \n",
    "    with h5py.File(hdf5_file, \"r\") as hdf:\n",
    "        print(\"\\nChecking FAIR Compliance for:\", hdf5_file)\n",
    "\n",
    "        # Step 1: Check Global Metadata\n",
    "        print(\"\\nGlobal Metadata Verification:\")\n",
    "        missing_metadata = []\n",
    "        for key in required_metadata:\n",
    "            if key in hdf.attrs:\n",
    "                print(f\"{key}: {hdf.attrs[key]}\")\n",
    "            else:\n",
    "                print(f\"MISSING: {key}\")\n",
    "                missing_metadata.append(key)\n",
    "\n",
    "        # Step 2: Check Dataset Structure\n",
    "        print(\"\\nDataset Verification:\")\n",
    "        if \"sensor_readings\" in hdf:\n",
    "            dataset = hdf[\"sensor_readings\"]\n",
    "            print(\"Dataset 'sensor_readings' found.\")\n",
    "\n",
    "            # Ensure column names metadata exists\n",
    "            if \"columns\" in dataset.attrs:\n",
    "                print(f\"Columns: {dataset.attrs['columns']}\")\n",
    "            else:\n",
    "                print(\"MISSING: Column metadata\")\n",
    "        else:\n",
    "            print(\"Dataset 'sensor_readings' NOT found!\")\n",
    "\n",
    "        # Step 3: Compute Checksum for Data Integrity\n",
    "        checksum = compute_checksum(hdf5_file)\n",
    "        print(\"\\nData Integrity Check:\")\n",
    "        print(f\"SHA256 Checksum: {checksum}\")\n",
    "\n",
    "        # Step 4: FAIR Compliance Summary\n",
    "        print(\"\\nFAIR Compliance Report:\")\n",
    "        if not missing_metadata and \"sensor_readings\" in hdf and \"columns\" in dataset.attrs:\n",
    "            print(\"The dataset meets FAIR principles!\")\n",
    "        else:\n",
    "            print(\"Some FAIR requirements are missing. Please update your metadata.\")\n",
    "\n",
    "# Run the compliance check on your HDF5 file\n",
    "hdf5_filename = \"NOx.h5\"  # Update this if needed\n",
    "check_fair_compliance(hdf5_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
