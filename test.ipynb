{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name  score\n",
      "a    Alice     85\n",
      "b      Bob     90\n",
      "c  Charlie     95\n",
      "d    David     80\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'score': [85, 90, 95, 80]\n",
    "}, index=['a', 'b', 'c', 'd'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name     Alice\n",
       "score       85\n",
       "Name: a, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    85\n",
       "b    90\n",
       "c    95\n",
       "d    80\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name     Alice\n",
       "score       85\n",
       "Name: a, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a      Alice\n",
       "b        Bob\n",
       "c    Charlie\n",
       "d      David\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What’s the output of this code?\n",
    "```Python\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3, 4])\n",
    "print(arr[arr > 2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Choose the correct statements about Python (multiple answers OK)\n",
    "    □ Python uses garbage collection to manage memory\n",
    "    □ Python variables must be declared with a type\n",
    "    □ Python functions can have default arguments\n",
    "    □ Python supports multi-threading without the GIL\n",
    "    □ Python’s with statement is used for context management\n",
    "\n",
    "    1,3,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What does this code output?\n",
    "```Python\n",
    "x = [1, 2, 3]\n",
    "y = x\n",
    "x.append(4)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SQL Logic\n",
    "You have a table employees with columns:\n",
    "\n",
    "* id (int)\n",
    "\n",
    "* name (varchar)\n",
    "\n",
    "* salary (float)\n",
    "\n",
    "* department (varchar)\n",
    "\n",
    "Write a SQL query to return the name of the employee(s) with the highest salary per department.\n",
    "\n",
    "```SQL\n",
    "SELECT e.name, e.salary, e.department\n",
    "FROM employees e\n",
    "JOIN (\n",
    "    SELECT department, MAX(salary) AS max_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    ") m ON e.department = m.department AND e.salary = m.max_salary;\n",
    "\n",
    "SELECT e.name, e.salary, e.department\n",
    "FROM employees e\n",
    "WHERE e.salary = (\n",
    "    SELECT MAX(salary)\n",
    "    FROM employees\n",
    "    WHERE department = e.department\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Code a function to flatten a nested list.\n",
    "Input:\n",
    "\n",
    "```Python\n",
    "[[1, 2], [3, 4, [5, 6]], 7]\n",
    "\n",
    "Expected output: [1, 2, 3, 4, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def flatten_it(lst):\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            result.extend(flatten_it(item))\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_it(lst):\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            yield from flatten_it(item)\n",
    "        else:\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [[1, 2], [3, 4, [5, 6]], 7]\n",
    "\n",
    "list(flatten_it(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What will this code return?\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'group': ['A', 'B', 'A', 'B'],\n",
    "    'value': [10, 20, 30, 40]\n",
    "})\n",
    "\n",
    "print(df.groupby('group')['value'].sum())\n",
    "\n",
    "□ A: 40, B: 60\n",
    "□ A: 20, B: 60\n",
    "□ A: 30, B: 40\n",
    "□ Error\n",
    "\n",
    "40, 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write a Bash command to count the number of lines in a large CSV file that contain the word “ERROR” (case-insensitive).\n",
    "\n",
    "```bash\n",
    "grep -i \"ERROR\" file.csv | wc -l\n",
    "```\n",
    "or without matching lines (just count)\n",
    "```bash\n",
    "grep -i -c \"ERROR\" file.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using requests, write a function that retries 3 times if the API call fails (status != 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_data(url):\n",
    "    try_number = 1\n",
    "    while try_number <= 3:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f'Try #{try_number}: API call failed with status {response.status_code}')\n",
    "            try_number += 1\n",
    "        else:\n",
    "            return response.json()\n",
    "        \n",
    "    raise Exception(f\"API call failed after 3 attempts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. What is the difference between INNER JOIN and LEFT JOIN in SQL? Provide an example if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "\n",
    "SELECT *\n",
    "FROM users\n",
    "INNER JOIN orders ON users.id = orders.user_id;\n",
    "```\n",
    "that will return only users who have placed orders (orders is another table with users_id).\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM users\n",
    "LEFT JOIN orders ON users.id = orders.user_id;\n",
    "```\n",
    "this one returns all users but for those who have not done any order there will be NULL or NAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using Python, how would you find the 5 most common words in a large text file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|      been|   88|\n",
      "|       has|   88|\n",
      "|   dataset|   87|\n",
      "|2023-02-27|   34|\n",
      "|        to|   30|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, col, lower\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('read_file') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.text('./data/HC-5059_ID06-LVP.txt')\n",
    "\n",
    "\n",
    "df_words = df.withColumn('words', split(col('value'), ' '))\n",
    "df_exploded = df_words.select(explode(col('words')).alias('word')) \\\n",
    "                      .withColumn('word', lower(col('word'))) \\\n",
    "                      .filter(col('word') != '')\n",
    "\n",
    "\n",
    "top_words = df_exploded.groupBy('word') \\\n",
    "    .count() \\\n",
    "    .orderBy(col('count').desc()) \\\n",
    "    .limit(5)\n",
    "\n",
    "top_words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Which of the following are Python libraries for data manipulation and analysis? (multiple OK)\n",
    "    □ NumPy\n",
    "    □ Pandas\n",
    "    □ TensorFlow\n",
    "    □ SQLAlchemy\n",
    "    □ OpenCV\n",
    "\n",
    "    Pandas, SQLalchemy, numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Write a Python function that reads a JSON file and returns all keys at the top level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_json_keys(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        return list(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Given the following log line:\n",
    "```pgsql\n",
    "2023-09-10 15:24:02, INFO, Job started by user: admin\n",
    "```\n",
    "Write a regular expression to extract the date, log level, and user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def date_log_user(log: str):\n",
    "    pattern = r\"(\\d{4}-\\d{2}-\\d{2}) \\d{2}:\\d{2}:\\d{2}, (\\w+), .*user: (\\w+)\"\n",
    "    match = re.search(pattern, log)\n",
    "    if match:\n",
    "        date, level, user = match.groups()\n",
    "        return [date, level, user]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023-09-10', 'INFO', 'admin']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = '2023-09-10 15:24:02, INFO, Job started by user: admin'\n",
    "date_log_user(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. What’s the difference between batch and stream processing? When would you use one over the other?\n",
    "\n",
    "In batch Processing data is collected over a period of time and then processed all at once (good for daily reports, dashboards)\n",
    "\n",
    "For Streaming data is collected live (and also processed in real-time) and can be used in IoT, fraud detection, when it is important to make decisions during data accusition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. What’s the difference between a DataFrame and a Series in pandas?\n",
    "\n",
    "Series is 1D, its a labeld 1D array\n",
    "\n",
    "DataFrame is a 2D Table, dictionary of Series with column names as keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Developer in Data & Machine Learning – Technical Test (LLM Focus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiple Choice: General Python and ML Concepts\n",
    "Which of the following statements are TRUE? (multiple answers OK)\n",
    "\n",
    "    □ A Python function can return multiple values\n",
    "    □ numpy is often used for data visualization\n",
    "    □ scikit-learn provides tools for deep learning\n",
    "    □ transformers is a library by Hugging Face\n",
    "    □ Tokenization is the process of converting strings into numerical IDs\n",
    "\n",
    "    1, 4, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What does the following code print?\n",
    "```python\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "qa = pipeline(\"question-answering\")\n",
    "context = \"Python is a popular programming language created by Guido van Rossum.\"\n",
    "\n",
    "result = qa(question=\"Who created Python?\", context=context)\n",
    "print(result[\"answer\"])\n",
    "```\n",
    "\n",
    "Guido van Rossum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write a function that takes a text string and returns the most frequent bigram (pair of consecutive words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent_bigram(text):\n",
    "\n",
    "    words = text.lower().split()\n",
    "    bigrams = list(zip(words, words[1:]))\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    most_common = bigram_counts.most_common(1)\n",
    "    \n",
    "    return most_common[0][0] if most_common else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What’s the difference between batch size, epoch, and learning rate in training ML models?\n",
    "\n",
    "batch size is a number of samples the model process at once before update the weights (activation and backpropagation)\n",
    "*   total number of points is 1000 and if batchsize is 100 then the model will do 10 calcultaions/updates\n",
    "-------\n",
    "epoch is total pass through the data pints\n",
    "*   so in the last example all 10 updates would be 1 epoch so multiple epochs are needed for good performance\n",
    "-------\n",
    "learning rate is a coefficient in weights backpropagation:\n",
    "*   small lead to smaler weight changes -> slower training (or no training at all)\n",
    "*   big lead to larger weight updates -> faster training (but can result in not convergency, overshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQL Logic\n",
    "You have a table users with columns:\n",
    "\n",
    "id (int)\n",
    "\n",
    "country (string)\n",
    "\n",
    "signup_date (date)\n",
    "\n",
    "Write a SQL query to return the number of signups per month in 2023, sorted chronologically.\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "  MONTH(signup_date) AS signup_month,\n",
    "  COUNT(*) AS total_users\n",
    "FROM users\n",
    "WHERE YEAR(signup_date) = 2023\n",
    "GROUP BY MONTH(signup_date)\n",
    "ORDER BY signup_month;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What’s the output of the following code?\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "print(x.T)\n",
    "\n",
    "\n",
    "[[1, 3], [2, 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Short Answer: What are attention mechanisms and why are they important in LLMs?\n",
    "\n",
    "it is when the model treats some data more focused than another (usually most important), it helps to grasp the context and capture dependences between words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. You have a CSV file with a text column containing customer reviews. Write a script to:\n",
    "Load the data with pandas\n",
    "\n",
    "Apply sentiment analysis using a Hugging Face pipeline\n",
    "\n",
    "Add the sentiment to a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def get_sentiment(data, review_column='review'):\n",
    "    sp = pipeline('sentiment-analysis')\n",
    "    data['sentiment_review'] = data[review_column].apply(\n",
    "        lambda x: sp(x)[0]['label'] if pd.notnull(x) else 'UNKNOWN'\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. What’s the difference between Fine-tuning and Prompt Engineering?\n",
    "\n",
    "well, prompt is a request (input you create) to get a desired output, it doesnt change the model itself\n",
    "Fine-tuning is for teaching the model anything new (by using another dataset: like teaching medicine a general LLM by feeding it medical books)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. You’re building a Q&A bot over internal company documents. What stack would you use to build a Retrieval-Augmented Generation (RAG) pipeline?\n",
    "Name the tools you’d use for:\n",
    "\n",
    "Document embedding :\n",
    "*   sentence-transformers or OpenAI Embeddings or Hugging Face Transformers\n",
    "\n",
    "Vector storage\n",
    "*   Pinecone, FAISS\n",
    "\n",
    "Retriever\n",
    "*   LangChain retrievers, Haystack (Finds the top-k most similar documents for a user query.)\n",
    "\n",
    "Language model\n",
    "*   openAI chatgpt, LLama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Write code to load an LLM from Hugging Face with 8-bit quantization (to save memory).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Define model, try 'tiiuae/falcon-7b'\n",
    "model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Given a dataframe of chat logs with columns: user_id, timestamp, message, write a function to compute the average number of messages per user per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def average_messages_per_user_per_day(df):\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    messages_per_user_per_day = df.groupby(['user_id', 'date']).size()\n",
    "    avg_messages = messages_per_user_per_day.groupby('user_id').mean()\n",
    "\n",
    "    return avg_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
