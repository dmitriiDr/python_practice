{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name  score\n",
      "a    Alice     85\n",
      "b      Bob     90\n",
      "c  Charlie     95\n",
      "d    David     80\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'score': [85, 90, 95, 80]\n",
    "}, index=['a', 'b', 'c', 'd'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name     Alice\n",
       "score       85\n",
       "Name: a, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    85\n",
       "b    90\n",
       "c    95\n",
       "d    80\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name     Alice\n",
       "score       85\n",
       "Name: a, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a      Alice\n",
       "b        Bob\n",
       "c    Charlie\n",
       "d      David\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What’s the output of this code?\n",
    "```Python\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3, 4])\n",
    "print(arr[arr > 2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Choose the correct statements about Python (multiple answers OK)\n",
    "    □ Python uses garbage collection to manage memory\n",
    "    □ Python variables must be declared with a type\n",
    "    □ Python functions can have default arguments\n",
    "    □ Python supports multi-threading without the GIL\n",
    "    □ Python’s with statement is used for context management\n",
    "\n",
    "    1,3,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What does this code output?\n",
    "```Python\n",
    "x = [1, 2, 3]\n",
    "y = x\n",
    "x.append(4)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SQL Logic\n",
    "You have a table employees with columns:\n",
    "\n",
    "* id (int)\n",
    "\n",
    "* name (varchar)\n",
    "\n",
    "* salary (float)\n",
    "\n",
    "* department (varchar)\n",
    "\n",
    "Write a SQL query to return the name of the employee(s) with the highest salary per department.\n",
    "\n",
    "```SQL\n",
    "SELECT e.name, e.salary, e.department\n",
    "FROM employees e\n",
    "JOIN (\n",
    "    SELECT department, MAX(salary) AS max_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    ") m ON e.department = m.department AND e.salary = m.max_salary;\n",
    "\n",
    "SELECT e.name, e.salary, e.department\n",
    "FROM employees e\n",
    "WHERE e.salary = (\n",
    "    SELECT MAX(salary)\n",
    "    FROM employees\n",
    "    WHERE department = e.department\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Code a function to flatten a nested list.\n",
    "Input:\n",
    "\n",
    "```Python\n",
    "[[1, 2], [3, 4, [5, 6]], 7]\n",
    "\n",
    "Expected output: [1, 2, 3, 4, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def flatten_it(lst):\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            result.extend(flatten_it(item))\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_it(lst):\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            yield from flatten_it(item)\n",
    "        else:\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [[1, 2], [3, 4, [5, 6]], 7]\n",
    "\n",
    "list(flatten_it(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What will this code return?\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'group': ['A', 'B', 'A', 'B'],\n",
    "    'value': [10, 20, 30, 40]\n",
    "})\n",
    "\n",
    "print(df.groupby('group')['value'].sum())\n",
    "\n",
    "□ A: 40, B: 60\n",
    "□ A: 20, B: 60\n",
    "□ A: 30, B: 40\n",
    "□ Error\n",
    "\n",
    "40, 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write a Bash command to count the number of lines in a large CSV file that contain the word “ERROR” (case-insensitive).\n",
    "\n",
    "```bash\n",
    "grep -i \"ERROR\" file.csv | wc -l\n",
    "```\n",
    "or without matching lines (just count)\n",
    "```bash\n",
    "grep -i -c \"ERROR\" file.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using requests, write a function that retries 3 times if the API call fails (status != 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_data(url):\n",
    "    try_number = 1\n",
    "    while try_number <= 3:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f'Try #{try_number}: API call failed with status {response.status_code}')\n",
    "            try_number += 1\n",
    "        else:\n",
    "            return response.json()\n",
    "        \n",
    "    raise Exception(f\"API call failed after 3 attempts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. What is the difference between INNER JOIN and LEFT JOIN in SQL? Provide an example if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "\n",
    "SELECT *\n",
    "FROM users\n",
    "INNER JOIN orders ON users.id = orders.user_id;\n",
    "```\n",
    "that will return only users who have placed orders (orders is another table with users_id).\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM users\n",
    "LEFT JOIN orders ON users.id = orders.user_id;\n",
    "```\n",
    "this one returns all users but for those who have not done any order there will be NULL or NAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using Python, how would you find the 5 most common words in a large text file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|      been|   88|\n",
      "|       has|   88|\n",
      "|   dataset|   87|\n",
      "|2023-02-27|   34|\n",
      "|        to|   30|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, col, lower\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('read_file') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.text('./data/HC-5059_ID06-LVP.txt')\n",
    "\n",
    "\n",
    "df_words = df.withColumn('words', split(col('value'), ' '))\n",
    "df_exploded = df_words.select(explode(col('words')).alias('word')) \\\n",
    "                      .withColumn('word', lower(col('word'))) \\\n",
    "                      .filter(col('word') != '')\n",
    "\n",
    "\n",
    "top_words = df_exploded.groupBy('word') \\\n",
    "    .count() \\\n",
    "    .orderBy(col('count').desc()) \\\n",
    "    .limit(5)\n",
    "\n",
    "top_words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Which of the following are Python libraries for data manipulation and analysis? (multiple OK)\n",
    "    □ NumPy\n",
    "    □ Pandas\n",
    "    □ TensorFlow\n",
    "    □ SQLAlchemy\n",
    "    □ OpenCV\n",
    "\n",
    "    Pandas, SQLalchemy, numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Write a Python function that reads a JSON file and returns all keys at the top level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_json_keys(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        return list(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Given the following log line:\n",
    "```pgsql\n",
    "2023-09-10 15:24:02, INFO, Job started by user: admin\n",
    "```\n",
    "Write a regular expression to extract the date, log level, and user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def date_log_user(log: str):\n",
    "    pattern = r\"(\\d{4}-\\d{2}-\\d{2}) \\d{2}:\\d{2}:\\d{2}, (\\w+), .*user: (\\w+)\"\n",
    "    match = re.search(pattern, log)\n",
    "    if match:\n",
    "        date, level, user = match.groups()\n",
    "        return [date, level, user]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023-09-10', 'INFO', 'admin']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = '2023-09-10 15:24:02, INFO, Job started by user: admin'\n",
    "date_log_user(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. What’s the difference between batch and stream processing? When would you use one over the other?\n",
    "\n",
    "In batch Processing data is collected over a period of time and then processed all at once (good for daily reports, dashboards)\n",
    "\n",
    "For Streaming data is collected live (and also processed in real-time) and can be used in IoT, fraud detection, when it is important to make decisions during data accusition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. What’s the difference between a DataFrame and a Series in pandas?\n",
    "\n",
    "Series is 1D, its a labeld 1D array\n",
    "\n",
    "DataFrame is a 2D Table, dictionary of Series with column names as keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Developer in Data & Machine Learning – Technical Test (LLM Focus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiple Choice: General Python and ML Concepts\n",
    "Which of the following statements are TRUE? (multiple answers OK)\n",
    "\n",
    "    □ A Python function can return multiple values\n",
    "    □ numpy is often used for data visualization\n",
    "    □ scikit-learn provides tools for deep learning\n",
    "    □ transformers is a library by Hugging Face\n",
    "    □ Tokenization is the process of converting strings into numerical IDs\n",
    "\n",
    "    1, 4, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What does the following code print?\n",
    "```python\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "qa = pipeline(\"question-answering\")\n",
    "context = \"Python is a popular programming language created by Guido van Rossum.\"\n",
    "\n",
    "result = qa(question=\"Who created Python?\", context=context)\n",
    "print(result[\"answer\"])\n",
    "```\n",
    "\n",
    "Guido van Rossum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write a function that takes a text string and returns the most frequent bigram (pair of consecutive words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent_bigram(text):\n",
    "\n",
    "    words = text.lower().split()\n",
    "    bigrams = list(zip(words, words[1:]))\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    most_common = bigram_counts.most_common(1)\n",
    "    \n",
    "    return most_common[0][0] if most_common else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What’s the difference between batch size, epoch, and learning rate in training ML models?\n",
    "\n",
    "batch size is a number of samples the model process at once before update the weights (activation and backpropagation)\n",
    "*   total number of points is 1000 and if batchsize is 100 then the model will do 10 calcultaions/updates\n",
    "-------\n",
    "epoch is total pass through the data pints\n",
    "*   so in the last example all 10 updates would be 1 epoch so multiple epochs are needed for good performance\n",
    "-------\n",
    "learning rate is a coefficient in weights backpropagation:\n",
    "*   small lead to smaler weight changes -> slower training (or no training at all)\n",
    "*   big lead to larger weight updates -> faster training (but can result in not convergency, overshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQL Logic\n",
    "You have a table users with columns:\n",
    "\n",
    "id (int)\n",
    "\n",
    "country (string)\n",
    "\n",
    "signup_date (date)\n",
    "\n",
    "Write a SQL query to return the number of signups per month in 2023, sorted chronologically.\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "  MONTH(signup_date) AS signup_month,\n",
    "  COUNT(*) AS total_users\n",
    "FROM users\n",
    "WHERE YEAR(signup_date) = 2023\n",
    "GROUP BY MONTH(signup_date)\n",
    "ORDER BY signup_month;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What’s the output of the following code?\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "print(x.T)\n",
    "\n",
    "\n",
    "[[1, 3], [2, 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Short Answer: What are attention mechanisms and why are they important in LLMs?\n",
    "\n",
    "it is when the model treats some data more focused than another (usually most important), it helps to grasp the context and capture dependences between words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. You have a CSV file with a text column containing customer reviews. Write a script to:\n",
    "Load the data with pandas\n",
    "\n",
    "Apply sentiment analysis using a Hugging Face pipeline\n",
    "\n",
    "Add the sentiment to a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def get_sentiment(data, review_column='review'):\n",
    "    sp = pipeline('sentiment-analysis')\n",
    "    data['sentiment_review'] = data[review_column].apply(\n",
    "        lambda x: sp(x)[0]['label'] if pd.notnull(x) else 'UNKNOWN'\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. What’s the difference between Fine-tuning and Prompt Engineering?\n",
    "\n",
    "well, prompt is a request (input you create) to get a desired output, it doesnt change the model itself\n",
    "Fine-tuning is for teaching the model anything new (by using another dataset: like teaching medicine a general LLM by feeding it medical books)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. You’re building a Q&A bot over internal company documents. What stack would you use to build a Retrieval-Augmented Generation (RAG) pipeline?\n",
    "Name the tools you’d use for:\n",
    "\n",
    "Document embedding :\n",
    "*   sentence-transformers or OpenAI Embeddings or Hugging Face Transformers\n",
    "\n",
    "Vector storage\n",
    "*   Pinecone, FAISS\n",
    "\n",
    "Retriever\n",
    "*   LangChain retrievers, Haystack (Finds the top-k most similar documents for a user query.)\n",
    "\n",
    "Language model\n",
    "*   openAI chatgpt, LLama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Write code to load an LLM from Hugging Face with 8-bit quantization (to save memory).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Define model, try 'tiiuae/falcon-7b'\n",
    "model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Given a dataframe of chat logs with columns: user_id, timestamp, message, write a function to compute the average number of messages per user per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def average_messages_per_user_per_day(df):\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    messages_per_user_per_day = df.groupby(['user_id', 'date']).size()\n",
    "    avg_messages = messages_per_user_per_day.groupby('user_id').mean()\n",
    "\n",
    "    return avg_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python: Multiple Choice\n",
    "Which of the following statements are TRUE? (Multiple answers possible)\n",
    "\n",
    "    □ Python supports list, set, and dictionary comprehensions\n",
    "    □ Dictionaries preserve insertion order (Python 3.7+)\n",
    "    □ with open() automatically closes files after reading\n",
    "    □ Tuples are mutable\n",
    "    □ None, False, and 0 are treated the same in conditional checks\n",
    "\n",
    "\n",
    "1, 3, 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File I/O\n",
    "Write a function to read a .csv file and return a list of dictionaries (one per row). Don’t use pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_read_csv(file_csv):\n",
    "    with open(file_csv, 'r') as file:\n",
    "        rows = [line.strip().split(',') for line in file]\n",
    "        header = rows[0]\n",
    "        values = list(zip(*rows[1:]))\n",
    "        \n",
    "        return dict(zip(header, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def my_read_csv(file_csv):\n",
    "    with open(file_csv, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        return [row for row in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. JSON Parsing\n",
    "Given the following JSON structure in a file. Write a function that extracts all employee names.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"employees\": [\n",
    "    {\"name\": \"Alice\", \"role\": \"engineer\"},\n",
    "    {\"name\": \"Bob\", \"role\": \"analyst\"}\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_employees(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        return [employee['name'] for employee in data['employees']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice', 'Bob']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_emploees('empl.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SQL\n",
    "You have a table orders with:\n",
    "\n",
    "order_id\n",
    "\n",
    "user_id\n",
    "\n",
    "order_date\n",
    "\n",
    "amount\n",
    "\n",
    "Write a SQL query to get the total amount spent per user in 2023, ordered from highest to lowest.\n",
    "\n",
    "```sql\n",
    "\n",
    "SELECT user_id, SUM(amount) as total_amount\n",
    "FROM orders\n",
    "WHERE YEAR(order_date) = 2023\n",
    "GROUP BY user_id\n",
    "ORDER BY total_amount DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regex\n",
    "Given the string:\n",
    "```python\n",
    "\"Order #1234 was placed on 2024-05-15 by user ID u678\"\n",
    "```\n",
    "\n",
    "Write a regex to extract:\n",
    "\n",
    "    The order number\n",
    "\n",
    "    The date\n",
    "\n",
    "    The user ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def order_info(line):\n",
    "    pattern = 'Order #(\\d+) was placed on (\\d{4}-\\d{2}-\\d{2}) by user ID (\\w+\\d+)'\n",
    "    return [match for match in re.findall(pattern, line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1234', '2024-05-15', 'u678')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_info(\"Order #1234 was placed on 2024-05-15 by user ID u678\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning\n",
    "Using pandas, write code to:\n",
    "\n",
    "    Load a CSV file with columns name, age, and income\n",
    "\n",
    "    Drop rows with missing age or income\n",
    "\n",
    "    Fill missing names with \"Unknown\"\n",
    "\n",
    "    Convert income to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(file_csv):\n",
    "    data = pd.read_csv(file_csv)\n",
    "    data = data.dropna(subset=['age','income'])\n",
    "    data['name'] = data['name'].fillna('Unknown')\n",
    "    data['income'] = data['income'].astype(float)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dictionaries\n",
    "Write a function that receives a list of strings and returns a dictionary with each unique string as key and its number of occurrences as value.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "[\"apple\", \"banana\", \"apple\"] → {\"apple\": 2, \"banana\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fruits(list_fruits):\n",
    "    result = {}\n",
    "    for fruit in list_fruits:\n",
    "        result[fruit] = result.get(fruit, 0) + 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 2, 'banana': 1}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_fruits([\"apple\", \"banana\", \"apple\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_fruits(list_fruits):\n",
    "    return dict(Counter(list_fruits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. List Comprehension Challenge\n",
    "From a list of integers, return a list of squares of even numbers between 0 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_squared(int_list):\n",
    "    return list(map(lambda x: x**2, filter(lambda x: x % 2 == 0 and x > 0 and x < 100, int_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 64, 4]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_squared([-1, 200, 3, 4, 5, 0, 0, 8, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_squared(int_list):\n",
    "    return [x**2 for x in int_list if x % 2 == 0 and x > 0 and x < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 64, 4]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_squared([-1, 200, 3, 4, 5, 0, 0, 8, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. REST API\n",
    "Use the requests module to send a GET request to:\n",
    "\n",
    "```Python\n",
    "`https://jsonplaceholder.typicode.com/posts`\n",
    "```\n",
    "Write code to fetch the data and print the title of the first 3 posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def first_3_titles(url):\n",
    "    response = requests.get(url).json()\n",
    "    return [user['title'] for user in response[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sunt aut facere repellat provident occaecati excepturi optio reprehenderit',\n",
       " 'qui est esse',\n",
       " 'ea molestias quasi exercitationem repellat qui ipsa sit aut']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_3_titles('https://jsonplaceholder.typicode.com/posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Command Line Tool (Bonus)\n",
    "Write a script that can be run as:\n",
    "\n",
    "```bash\n",
    "python stats.py data.csv --column=age\n",
    "```\n",
    "It should print the mean, min, and max of the specified column from a CSV.\n",
    "\n",
    "```python\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def get_stats(data, col):\n",
    "    print(f'Statistic for {col}\\nMean: {data[col].mean()}\\nMin: {data[col].min()}\\nMax: {data[col].max()}')\n",
    "\n",
    "def main(input, col):\n",
    "    data = pd.read_csv(input)\n",
    "    get_stats(data, col)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('csv_file', type=str, help='path to the csv file')\n",
    "    parser.add_argument('--column', type=str, required=True, help='select a column')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input = Path(args.csv_file)\n",
    "    col = args.column\n",
    "\n",
    "    main(input, col)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiple Choice: Data Types & Structures\n",
    "Which of the following are mutable types in Python?\n",
    "\n",
    "    □ list\n",
    "    □ dict\n",
    "    □ str\n",
    "    □ set\n",
    "    □ tuple\n",
    "\n",
    "1, 2, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function Logic\n",
    "Write a function that accepts a string and returns True if it’s a valid IPv4 address, otherwise False. Example of valid: \"192.168.1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid_IP(str_ip):\n",
    "    ip = str_ip.split('.')\n",
    "    if len(ip) != 4:\n",
    "        return False\n",
    "    for number in ip:\n",
    "        if not number.isdigit():\n",
    "            return False\n",
    "        if not 0 <= int(number) <= 255:\n",
    "            return False\n",
    "        if number != '0' and number.startswith('0'):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_valid_IP('16.5.255.2a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CSV Processing\n",
    "You are given a CSV file with the following columns:\n",
    "\n",
    "```python\n",
    "user_id, session_length, timestamp\n",
    "```\n",
    "\n",
    "Write code to:\n",
    "\n",
    "    Load the CSV with pandas\n",
    "\n",
    "    Group by user_id and compute the average session length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_mean_session_length(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    avg_session = data.groupby('user_id')['session_length'].mean()\n",
    "    return avg_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Joining\n",
    "You have two CSVs:\n",
    "\n",
    "users.csv\n",
    "\n",
    "```yaml\n",
    "user_id, name\n",
    "1, Alice\n",
    "2, Bob\n",
    "```\n",
    "\n",
    "logins.csv\n",
    "```yaml\n",
    "user_id, login_time\n",
    "1, 2024-01-01\n",
    "1, 2024-01-02\n",
    "2, 2024-01-01\n",
    "```\n",
    "Write code to merge the data and count logins per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 =pd.DataFrame({'user_id': (1,2), 'name':('Alice', 'Bob')})\n",
    "df2 =pd.DataFrame({'user_id': (1,1,2), 'login_time': ('2024-01-01', '2024-01-02', '2024-01-01')})\n",
    "\n",
    "def merge_count(df_users, df_logins):\n",
    "    login_counts = df_logins.groupby('user_id').size().reset_index(name='total_logins')\n",
    "    result = df_users.merge(login_counts, on='user_id', how='left')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>total_logins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id   name  total_logins\n",
       "0        1  Alice             2\n",
       "1        2    Bob             1"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_count(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. String Handling\n",
    "Write a function that takes a sentence and returns the number of unique words, case-insensitive, ignoring punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def number_unique_words(input_str):\n",
    "    words = re.findall(r'\\b\\w+\\b', input_str.lower())\n",
    "    return len(set(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_unique_words('hello? ho.w are you!!a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SQL\n",
    "Given table files with:\n",
    "\n",
    "filename\n",
    "\n",
    "size_kb\n",
    "\n",
    "upload_date\n",
    "\n",
    "Write a SQL query to return the average file size per month in 2024.\n",
    "\n",
    "```sql\n",
    "SELECT AVG(size_kb) as average_size, MONTH(upload_date) as upload_month\n",
    "FROM files\n",
    "WHERE YEAR(upload_date) = 2024\n",
    "GROUP BY upload_month\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time & Date\n",
    "\n",
    "Using Python's datetime module, write a function that returns the number of weekdays between two given dates (as strings in \"YYYY-MM-DD\" format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-04 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = '2024-02-04'\n",
    "end = '2024-05-04'\n",
    "print(datetime.datetime(int(start.split('-')[0]),int(start.split('-')[1]),int(start.split('-')[2])))\n",
    "\n",
    "# def get_weekdays(start, end):\n",
    "#     datetime.datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2024, 2, 4]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int(x) for x in start.split('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def count_weekdays(start_date: str, end_date: str):\n",
    "\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "    \n",
    "    if start > end:\n",
    "        start, end = end, start\n",
    "\n",
    "    weekday_count = 0\n",
    "    current_day = start\n",
    "    \n",
    "    while current_day <= end:\n",
    "        if current_day.weekday() < 5:\n",
    "            weekday_count += 1\n",
    "        current_day += timedelta(days=1)\n",
    "    \n",
    "    return weekday_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Set 1: Navigation & Files\n",
    "\n",
    "\n",
    "    1. Show your current directory path\n",
    "```bash\n",
    "    pwd\n",
    "```\n",
    "    2. Create a directory called bash_practice and move into it\n",
    "```bash\n",
    "    mkdir bash_practice && cd bash_practice\n",
    "```\n",
    "    3. Inside bash_practice, create an empty file named log.txt\n",
    "```bash\n",
    "    touch log.txt\n",
    "```\n",
    "    4. Copy log.txt to a new file called log_backup.txt\n",
    "```bash\n",
    "    cp log.txt log_backup.txt\n",
    "```\n",
    "    5. Rename log.txt to main_log.txt\n",
    "```bash\n",
    "    mv log.txt main_log.txt\n",
    "```\n",
    "    6. Remove the log_backup.txt file\n",
    "```bash\n",
    "    rm log_backup.txt\n",
    "```\n",
    "    7. Go one directory up and remove the bash_practice folder and everything inside\n",
    "```bash\n",
    "    cd .. && rm -r bash_practice/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Set 2: Variables & Printing\n",
    "\n",
    "Create a variable called city and assign it your favorite city\n",
    "→ Then print: \"I want to visit (city)\"\n",
    "```bash\n",
    "    city='London'&& echo \"I want to visit $city\"\n",
    "```\n",
    "Create two variables, first=\"Dmitrii\" and last=\"Ivanov\"\n",
    "→ Then print: \"Hello, Dmitrii Ivanov!\" (using those variables)\n",
    "\n",
    "```bash\n",
    "    first='Dmitrii' && last='Ivanov' && echo \"Hello, $first $last!\"\n",
    "```\n",
    "\n",
    "Try using echo with a command substitution\n",
    "→ Print the current date with this structure:\n",
    "\"Today is: (current date)\"\n",
    "\n",
    "```bash\n",
    "    echo \"Today is $(date)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Set 3: Conditionals & Loops\n",
    "\n",
    "    1. Conditionals\n",
    "Ask the user for their favorite programming language. If they say \"Python\", print \"Good choice!\". Otherwise, print \"Try Python, it’s awesome!\"\n",
    "\n",
    "```bash\n",
    "read lang\n",
    "if [ \"$lang\" == \"Python\" ]; then \n",
    "echo \"Good choice!\"\n",
    "else echo \"Try Python, it's awesome!\"\n",
    "fi\n",
    "```\n",
    "    2. For loop\n",
    "Write a loop that prints the numbers from 5 to 1 (in reverse)\n",
    "Hint: use {5..1} with a step\n",
    "\n",
    "```bash\n",
    "for i in {5..1..-1}; do\n",
    "echo \"Number $i\"\n",
    "done\n",
    "```\n",
    "    3. While loop\n",
    "Create a while loop that counts from 1 to 3 and prints \"Counting: X\"\n",
    "```bash\n",
    "counter=1\n",
    "while [ $counter -le 3 ]; do\n",
    "echo \"Counting: $counter\"\n",
    "((counter++))\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Set 4: Simple Bash Script\n",
    "\n",
    "Create a file called greet.sh\n",
    "\n",
    "Add the following logic inside:\n",
    "\n",
    "    Ask the user for their name\n",
    "\n",
    "If the name is Dmitrii, print: \"Welcome back, Dmitrii!\"\n",
    "\n",
    "Else, print: \"Hello, (name)!\"\n",
    "\n",
    "    Make it executable and run it\n",
    "\n",
    "```bash\n",
    "    touch greet.sh && printf '#!/bin/bash\\necho \"What'\\''s your name?\"\\nread name\\nif [ \"$name\" == \"Dmitrii\" ]; then\\n  echo \"Welcome back, Dmitrii!\"\\nelse\\n  echo \"Hello $name!\"\\nfi\\n' > greet.sh && chmod +x greet.sh && ./greet.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Tools Exercises\n",
    "\n",
    "```bash\n",
    "cat <<EOF > sample.txt\n",
    "Name Age Score\n",
    "Alice 30 85\n",
    "Bob 25 92\n",
    "Charlie 35 78\n",
    "Dmitrii 34 100\n",
    "Eve 28 66\n",
    "EOF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grep Challenges\n",
    "    Find all lines that contain the name Dmitrii\n",
    "```bash\n",
    "    grep \"Dmitrii\" sample.txt\n",
    "```\n",
    "    Find all lines with a score of at least 9 (just try grep without regex for now)\n",
    "```bash\n",
    "    awk '$3 >= 90 {print $0}' sample.txt\n",
    "\n",
    "    grep -E \"9[0-9]|100\" sample.txt\n",
    "```\n",
    "    Find all lines that contain names with capital C\n",
    "```bash\n",
    "    grep \"\\bC\" sample.txt\n",
    "\n",
    "    grep \"^C\" sample.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sed Challenges\n",
    "\n",
    "    Replace the name Bob with Robert\n",
    "```bash\n",
    "sed 's/Bob/Robert/' sample.txt\n",
    "```\n",
    "    Delete the first line of the file\n",
    "```bash\n",
    "sed '1d' sample.txt\n",
    "```\n",
    "    Replace all spaces with commas (CSV format!)\n",
    "```bash\n",
    "sed 's/ /,/g' sample.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## awk Challenges\n",
    "\n",
    "    Print only the names\n",
    "```bash\n",
    "awk '{print $1}' sample.txt\n",
    "```\n",
    "    Print names and scores of people with a score above 80\n",
    "```bash\n",
    "awk '$3 > 80 {print $1, $3}' sample.txt\n",
    "```\n",
    "    Print all names and add 10 years to their age\n",
    "```bash\n",
    "awk '{print $1, $2 + 10}' sample.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combo Challenge: Clean + Filter + Transform\n",
    "\n",
    "You have a file called people.txt with messy data:\n",
    "\n",
    "```bash\n",
    "#name age score\n",
    " Alice   30   85\n",
    "Bob 25 92\n",
    " Charlie 35 78\n",
    "Dmitrii 34 100\n",
    "Eve  28   66\n",
    "Dan   23 88\n",
    "```\n",
    "    Remove the header line (the one starting with #)\n",
    "\n",
    "    Trim all extra spaces (make fields nicely separated by a single space)\n",
    "\n",
    "    Keep only the people whose names start with D\n",
    "\n",
    "    Add 5 years to their age\n",
    "\n",
    "    Convert the output to a CSV format: Name,NewAge,Score\n",
    "    \n",
    "```bash\n",
    "sed -E -e '1d' -e 's/^ *//g' -e 's/  +/ /g' -n -e '/^D/p' people.txt | awk '{print $1 \",\" $2+5 \",\" $3}' \n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASH Exercise #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Log Analyzer\n",
    "File: server.log\n",
    "\n",
    "```sql\n",
    "[INFO] 2024-04-01 Server started\n",
    "[ERROR] 2024-04-01 Failed to connect to DB\n",
    "[WARNING] 2024-04-02 Low disk space\n",
    "[ERROR] 2024-04-03 Timeout on port 8080\n",
    "[INFO] 2024-04-03 Connection restored\n",
    "```\n",
    "\n",
    "```bash\n",
    "cat <<EOF > server.log\n",
    "[INFO] 2024-04-01 Server started\n",
    "[ERROR] 2024-04-01 Failed to connect to DB\n",
    "[WARNING] 2024-04-02 Low disk space\n",
    "[ERROR] 2024-04-03 Timeout on port 8080\n",
    "[INFO] 2024-04-03 Connection restored\n",
    "EOF\n",
    "```\n",
    "\n",
    "* Extract only the lines with [ERROR]\n",
    "\n",
    "```bash\n",
    "grep \"ERROR\" server.log\n",
    "```\n",
    "* Count how many error lines there are\n",
    "```bash\n",
    "grep -c \"ERROR\" server.log\n",
    "\n",
    "grep \"ERROR\" server.log | wc -l\n",
    "```\n",
    "* Extract the date and message from each error line in CSV format like: 2024-04-01,Failed to connect to DB\n",
    "```bash\n",
    "grep \"ERROR\" server.log | sed -E 's/.* ([0-9-]+) (.*)/\\1,\\2/'\n",
    "\n",
    "awk '/ERROR/ {print $2 \",\" substr($0, index($0,$3))}' server.log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASH exercise #2\n",
    "## Temperature Monitor\n",
    "File: temps.txt\n",
    "```sql\n",
    "2024-04-01 23°C  \n",
    "2024-04-02 19°C  \n",
    "2024-04-03 28°C  \n",
    "2024-04-04 16°C  \n",
    "2024-04-05 21°C\n",
    "```\n",
    "to create:\n",
    "```bash\n",
    "cat <<EOF > temps.txt\n",
    "2024-04-01 23°C\n",
    "2024-04-02 19°C\n",
    "2024-04-03 28°C\n",
    "2024-04-04 16°C\n",
    "2024-04-05 21°C\n",
    "EOF\n",
    "```\n",
    "\n",
    "*   Extract lines where the temperature is below 20°C\n",
    "```bash\n",
    "sed -E 's/°C//g' temps.txt | awk '$2 < 20 {print $0}'\n",
    "```\n",
    "*   Output only the date and temperature (without the °C), in CSV format:\n",
    "Example: 2024-04-02,19\n",
    "```bash\n",
    "sed -E 's/°C//g' temps.txt | awk '{print $1 \",\" $2}'\n",
    "```\n",
    "*   Count how many cold days (<20°C)\n",
    "```bash\n",
    "sed -E 's/°C//g' temps.txt | awk '$2 < 20 {print $0}' | wc -l\n",
    "\n",
    "awk '{gsub(\"°C\",\"\"); if ($2 < 20) count++} END {print count}' temps.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASH exercise #2\n",
    "## \"Who’s the most active?\"\n",
    "File: logins.txt\n",
    "Imagine this is a log of who logged in and how many times:\n",
    "\n",
    "```sql\n",
    "alice\n",
    "bob\n",
    "alice\n",
    "charlie\n",
    "bob\n",
    "bob\n",
    "```\n",
    "to create:\n",
    "```bash\n",
    "cat <<EOF > logins.txt\n",
    "alice\n",
    "bob\n",
    "alice\n",
    "charlie\n",
    "bob\n",
    "bob\n",
    "EOF\n",
    "```\n",
    "*   Count how many times each user appears\n",
    "*   Sort users by login count (descending)\n",
    "*   Print only the most active user\n",
    "```bash\n",
    "sort logins.txt | uniq -c | sort -nr | head -1\n",
    "\n",
    "sort logins.txt | uniq -c | sort -nr | head -1 | awk '{print $2}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASH exercise #3\n",
    "## Join + Summarize\n",
    "\n",
    "Files:\n",
    "\n",
    "users.csv\n",
    "```sql\n",
    "id,name\n",
    "1,Alice\n",
    "2,Bob\n",
    "3,Charlie\n",
    "4,Dmitrii\n",
    "5,Eve\n",
    "```\n",
    "\n",
    "```bash\n",
    "cat <<EOF > users.csv\n",
    "id,name\n",
    "1,Alice\n",
    "2,Bob\n",
    "3,Charlie\n",
    "4,Dmitrii\n",
    "5,Eve\n",
    "EOF\n",
    "```\n",
    "logins.csv\n",
    "```sql\n",
    "user_id,date\n",
    "2,2024-04-01\n",
    "4,2024-04-01\n",
    "2,2024-04-02\n",
    "3,2024-04-02\n",
    "2,2024-04-03\n",
    "4,2024-04-03\n",
    "```\n",
    "```bash\n",
    "cat <<EOF > logins.csv\n",
    "user_id,date\n",
    "2,2024-04-01\n",
    "4,2024-04-01\n",
    "2,2024-04-02\n",
    "3,2024-04-02\n",
    "2,2024-04-03\n",
    "4,2024-04-03\n",
    "EOF\n",
    "``` \n",
    "\n",
    "Join logins.csv with users.csv to match user_id with their name\n",
    "\n",
    "*   Final columns: date,name\n",
    "\n",
    "*   Skip headers\n",
    "```bash\n",
    "join -t, -1 1 -2 1 <(sort logins.csv) <(sort users.csv) | awk -F',' '{print $2 \" \" $3}'\n",
    "```\n",
    "Count logins per user name\n",
    "Sort by most logins, print only the most active user\n",
    "```bash\n",
    "join -t, -1 1 -2 1 <(sort logins.csv) <(sort users.csv) | awk -F',' '{print $3}' | uniq -c | sort -nr | awk '{print $2}' | head -1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
